Low-power potential of mixed-signal design makes it an alluring option to accelerate Deep Neural Networks (DNNs). However, mixed-signal circuitry suffers from limited range for information encoding, susceptibility to noise, and Analog to Digital (AD) conversion overheads. This paper aims to address these challenges by offering and leveraging the insight that a vector dot-product (the basic operation in DNNs) can be bit-partitioned into groups of spatially parallel low-bitwidth operations, and interleaved across multiple elements of the vectors. As such, the building blocks of our accelerator become a group of wide, yet low-bitwidth multiply-accumulate units that operate in the analog domain and share a single AD converter. The low-bitwidth operation tackles the encoding range limitation and facilitates noise mitigation. Moreover, we utilize the switched-capacitor design for our bit-level reformulation of DNN operations. The proposed switched-capacitor circuitry performs the group multiplications in the charge domain and accumulates the results of the group in its capacitors over multiple cycles. The capacitive accumulation combined with wide bit-partitioned operations alleviate the need for AD conversion per operation. With such mathematical reformulation and its switched-capacitor implementation, we define a 3D-stacked microarchitecture, dubbed tlass 1 footnote 1 1 footnote 1 tlass: B it-Partitioned and I nterleaved Hi erarchy of W ide Acceleration through E lectrical Charge - pronounced Bee Hive - that leverages clustering and hierarchical design to best utilize power-efficiency of the mixed-signal domain and 3D stacking. For ten DNN benchmarks, tlass delivers speedupOverTetris speedup over a leading purely-digital 3D-stacked accelerator etris, with a mere of less than 0.5 accuracy loss achieved by careful treatment of noise, computation error, and various forms of variation. Compared to RTX 2080 TI with tensor cores and Titan Xp GPUs, all with 8-bit execution, tlass offers perfWattOverRTX and perfWattOverTitan higher Performance-per-Watt, respectively. ihiwe also outperforms other leading digital and analog accelerators in power efficiency. The results suggest that tlass is an effective initial step in a road that combines mathematics, circuits, and architecture.