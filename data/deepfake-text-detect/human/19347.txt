I don't see a point to feminism besides to hate men. The feminists that I know all claim that society, United States of America, oppress them because society is mainly geared towards men. In reality I think women have more opportunities than men, more available scholarships, and more support groups. edit: great points from everyone. Sorry I was so broad with the term "rights." I meant it in both cultural advantages and also in the legal meaning.