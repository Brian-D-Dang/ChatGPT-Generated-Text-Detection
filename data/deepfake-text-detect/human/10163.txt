The word 'natural' implies something derived from nature, separate from humans. But why is there this distinction? Humans are obviously a part of and are derived from nature. So unless you're under any religious persuasion, the word shouldn't have any meaning. It seems outdated and non-applicable. For instance it's meaningless to me when someone says 'it's only natural'. What isn't natural? Saying 'this happens in nature' is the same as saying 'this happens'. Because everything happens in nature. And so on.