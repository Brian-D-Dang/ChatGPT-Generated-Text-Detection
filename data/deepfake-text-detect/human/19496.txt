An employer should have no responsibility to provide their employees with health insurance. Requiring them to do so needlessly burdens the employers freedoms while forcing employers to act as an agent of the government in enforcing the individual mandate. The American people and employer liberties would be far better served if employee compensation was increased and employers were left to shop for their own health insurance. This may require en equivalent bump to the minimum wage to make up for an employers contribution, but other than that I don't see any room for ill effects on the system. It's a simple accounting game... instead of my employer giving the money to the insurance company, they give it to me and then I give it to the insurance company. Also note... I'm not saying that employers should be required NOT to give health insurance as a form of compensation. Obviously there are some cases where an employer has a vested interest in the health of its workers. Some are targeting employees who see it as a perk that outweighs the cost. There is no reason they shouldn't be allowed to do so. EDIT: health INSURANCE, not health CARE