Okay, so I'm a 17 year old guy living in a middle-class family in Sydney, Australia. Being from a developed Western nation, my views toward Africa consist of association with war, disease, poverty and criminals. Now I know this can't be wholly true, and seeing as I finish high school later this year, Africa is definitely somewhere I'd consider travelling to one day, if only my mind could be changed about me coming back in a body bag. To clarify, I'm not racist in the slightest, I know theres plenty of good people in Africa, but being a student of legal studies I constantly hear about child soldiers, human trafficking and civil wars running rampant through the African continent. So its through my schooling, and the media in Australia (that can often talk about violence in Africa and the Middle East a lot more than in any other parts of the world), that has led me to these views. The idea of backpacking across Europe on my own is a daunting thought, but doing it across Africa, basically an unknown wilderness to me, is doubly as scary. So reddit, please change my view on Africa.