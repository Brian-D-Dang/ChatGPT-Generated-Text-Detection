So I work in a mall in florida, often I have customers that don't speak english at all and get upset with me for not speaking their language. Now I know that some of these people are tourists but growing up here in my teen years I know that I have peers that have to translate everything for their parents because they don't want to learn the language that is spoken in america. I know that America does not have a national language but does in some states. Am i wrong for believing people who come to america should speak english (Please exclude tourists from this question)