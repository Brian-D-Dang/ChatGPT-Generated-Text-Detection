For clarity, I'm talking about police forces in Western countries such as the United States, Britain, Canada and Australia. Over the last 40 years the policing field has become increasingly diverse, multicultural and less dominated by men. In addition to this, the focus has shifted considerably toward collaboration with and immersion in the community and 'community policing'. I believe Police officers are now better trained, better disciplined and better equipped than ever before. Furthermore I believe that most officers are trustworthy and honest. So without resorting to anecdotal evidence, how are the police as a body any worse than the public at large?