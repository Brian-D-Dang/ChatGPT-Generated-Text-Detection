I read a series of statistics with one of my High School English teachers who loves to have his students see the world in new ways. The point of the essay was to show that men are not necessarily better off than women in society. I have since decided that is not true that women are better off, but still wonder about the first part of the essay (I believe it was called the war on boys) talking about public education as well as college. The statistics, which I can't conjure so you just have to trust in my memory, mostly dealt with how more teachers are women and therefore most boys are misunderstood. The early years of education where boys were rowdy created tension where teachers assumed they were just acting out, rather than following their instincts and using that time to explore their physical domain. Throughout their education, boys are told their handwriting is bad, find their test scores to be lower, and their grades are generally not as good. It quoted a study saying that SAT scores are lower for women, but it turned out it was only because way more women were taking it even if they were at risk for low scores. The men in the high schools simply didn't think they were good enough to take the test. 60 percent of college admissions in the United States are women and much more than that in the case of graduate schools. There are more stats I can come up within the discussions of this subject. So yeah change my view