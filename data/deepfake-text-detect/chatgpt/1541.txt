I believe that, at the end of the day, America (and to a lesser extent, the UK and USSR) ultimately lost the second World War in every way that matters. I think people have bad memories or are too young to remember what happened outside their borders for a while after 1945 We did win though right?