I believe that, at the end of the day, America (and to a lesser extent, the UK and USSR) ultimately lost the second World War in every way that matters. The United States was never more than a ragtag opposition force when it came down to fighting Nazi Germany; it had no chance against the sheer might of WWII. So how do you explain the Allies winning? Edit: If you want to argue this I'll take you on. But based off what I've learned from Reddit replies today it seems like the US got more support than they deserve. Supporting Japan is just as much an American act as 40 years of military aid. We didn't save them like we saved the USSR or China but we did give them some help.