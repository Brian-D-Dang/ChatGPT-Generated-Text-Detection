I believe that, at the end of the day, America (and to a lesser extent, the UK and USSR) ultimately lost the second World War in every way that matters. Now this is just my opinion here but I'm willing to bet money on it: if you ask most people about their memories of WWII they will talk about two things; The war itself, or what happened afterwards. Even those who don't have any direct family connections with that time tend to be more readily able to recall specific facts around when certain events occurred than others - why? Because we all remember where we were during . We know what our lives was like before then and how it changed after. And because there are so many movies associated with these years, as well as music from them that we grew up listening too, they become cultural staples that simply stick out above everything else coming along between 1950 and 1980. This has made certain aspects of the Second World War almost universally known across generations. But let me remind everyone reading this article now, even though the Second World War ended less than 70 years ago in the US, for most Europeans this was an event that lasted 20 years and produced millions upon millions of casualties. There are still people alive today who witnessed Hitler's rise to power first hand, or fought against him during some of his campaigns. Yes, the Allies won both wars in the long run, but only by decades later did Europe finally get back down to normal levels of violence. In short, while the results may seem clear cut, consider that "winning" isn't always the same thing as being right.