I'm a programmer, and I am really interested in data science and machine learning. I think it started at my current job, where occasionally I was told to scrape websites for certain information (like the top games played on Steam, or the most recent news headlines from sites like CNN). When I realized that I could do this kind of thing myself without using an API, and with less effort than trying to find some random person who would spend hours scraping a website by hand, I became obsessed! It seems so simple when you try to explain it: just write a program to go through all the pages one-by-one until they spit out what you want them to. But then you start finding gaps in how your data is structured, weird things happen if there are any errors, etc. So writing these programs can take more time than actually doing the work manually might have taken me...but since I didn't waste any human time doing the "hard" part, the amount of satisfaction gained by successfully completing such tasks far outweighs those times where something screws up. And now that I know how, it's even easier knowing I don't need someone else's help to deal with whatever problems arise! Over the past few years, I've tried many different techniques to accomplish this task. The first version of GreaseMonkey scripts were pretty good; but as soon as I needed anything more complex than simply adding new fields to existing tables on a webpage, either I had to rewrite everything entirely because the way the script worked depended entirely upon getting a string returned from Javascript, or use JavaScript libraries which still left me having to convert strings back into values JS understood - not exactly what I wanted. Then came XULRunner, which let me run web browser applications locally instead of over HTTP... except it was only supported for Internet Explorer, making it useless unless I felt like installing IE6 again for Windows XP computers. Nothing against people who prefer IE6, mind you just wasn't much point spending 30 shipping costs just to be able to see a page I already knew existed somewhere online. Finally, after about 6 months of searching around various forums and wondering why nobody made it easy yet, I decided to give nodejs a shot. Now? If the site doesn't support NodeJS, there must be no reason for anyone apart from Microsoft employees to visit the site anyway, right?? Right?! Well, maybe. But it still does everything I asked it to do: allows me send requests to the server (or fetch the entire database!), make AJAX calls to loadedit interactively, filters results based on custom user attributes, formats logs requestresponse info automatically via log4j, sends notifications whenever any action takes place, AND IT ALL RUNS ON THE SAME CPU AS MY COMPUTER!! Plus the fact that I use Emacs for editing makes development quicker than ever before - nevermind being able to debug every single line of code while looking at both source files simultaneously! All in all its been very welcome change from my previous experiences. So yeah, once you're familiar enough with programming languages to be considered experienced, definitely check out NodeJS. Just remember that unless you have quite a bit experience working with databases, other types of servers, http protocols, etc., you probably won't get very far without figuring stuff out along the way. Don't expect to set up a complete system without studying books on topics which will come naturally to others (that's something even programmers struggle with sometimes), because you'll constantly hit walls that aren't easily overcome unless you learn ways around them.