I believe that, at the end of the day, America (and to a lesser extent, the UK and USSR) ultimately lost the second World War in every way that matters. It's not like they were even winning before then... This is so true, but just look what history has taught us about Germany during the final days there. They had no idea how to come back from it all.