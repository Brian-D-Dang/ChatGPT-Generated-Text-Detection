I'm a lifelong veggie, and I've been vegan for a while in the past although I'm not anymore. I don't believe you can make a rock solid case for being vegetarian when it comes to health benefits since most people are actually healthier on meat-based diets anyway, but if there's some benefit for plant based foods then go ahead. But overall which is better? A diet that's mostly plants or one with lots of meat?