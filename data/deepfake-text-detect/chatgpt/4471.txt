I'm going to clarify that by'Trigger Warnings', I'm referring to a tagdisclaimer of some sort that warns the audience of content that could reasonably produce an involuntary negative response (eg: containing violent imagery). What is there and currently implemented? Here's what exists as far as I know. The current default status for posts on the front page includes both "adult" or "mature" tags, but these are applied silently when you edit your post; they're not added to any text rendered in the preview window before it shows up live. They're also not shown when viewing other people's posts either directly via RSS or using algorithmic feeds like Google Reader, the latter being true even if those users have opted out of seeing such information from their own feed (and thus never see them on Stories or anywhere else). Additionally, since we've figured out how to include custom fields per-post and apply meta data (which can be used to inform things like search), moderators will sometimes employ these techniques to allow us to mark certain pieces of content with one or more specific labels. These may be something as simple as tagging all items in a discussion threadgroup conversation with the appropriate name (s) - the OP might use this technique themselves simply because he doesn't want members to stumble upon the contents accidentally - or they may refer to stricter company policies like whether photos contain nudity or sexual acts. Moderators would then make sure to enforce these rules when editing new posts within discussions related to said topics. The use of meta data seems quite powerful here, allowing us to assign additional categories (such as 'Not Safe For Work', etc.), but once again, they don't render in the posted text itself so no human should ever misread it unless they just plain miss whatever subtle hint was provided;P There's another feature called moderator boilerplate which provides us with a button to place at the bottom of each comment, or under the last line of every reply box, where after clicking it pops up a small dialog box informing readers about important guidelines (like what kinds of content our community expects, what kind of language is acceptable, and others) that everyone needs to adhere to. This has been included both manually by humans doing manual review, and automatically through automated methods via AI systems like the aforementioned Community Guidelines bot. We tend to stick to very general terms like x] Adult Content' rather than getting into details like only images depicting genitals, but the concept generally remains similar across different sites of ours. Having highlighted the fact that these do indeed exist, let me point out that while this method certainly works well enough, the problem still arises occasionally - sometimes users forget to apply the adult label to potentially harmful material, perhaps believing otherwise innocuous content to actually fall under the category instead, or misunderstanding the actual intent behind its inclusion. And unfortunately, applying filter bubbles isn't always sufficient either:P While most major social networks (including Facebook, Twitter and Tumblr) offer ways to block individual friends from seeing particular types of content entirely, Reddit still allows users to select specific groups by selecting multiple checkboxes next to different links on the sidebar (the full list of subreddits available to anyone who visits rall is almost incomprehensible), though whether or not these sorts of filters help improve user experience overall depends entirely on how much you trust your fellow redditors (for instance, rPolitics can easily turn into a cesspool pretty quickly). In practice, adding these markers to posts does seem to create a positive effect for a lot of reasons, including providing better context for other users browsing through someone's submissions, helping keep things cleaner for lowres users placing ads, and making contributors feel less compelled to scrape sloppy looking ones right off the bat.