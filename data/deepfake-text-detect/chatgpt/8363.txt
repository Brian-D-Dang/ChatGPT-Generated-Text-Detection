Greetings! I've long held this belief and constantly fought with more patriotic members of my country (Hungary) over it, as they say it will lead to cultural decay. However, as the US has been in decline for some time now while Hungary is still growing economically every year since joining EU - maybe we should think about re-inventing ourselves culturally also? After all there are other countries besides America which have a unique culture deriving from roots so deep you can almost feel them: Brazil or Mexico come instantly into mind... And none seem too bad at present either by any measure where being American hasn't become synonymous only meaning wealth these days:)