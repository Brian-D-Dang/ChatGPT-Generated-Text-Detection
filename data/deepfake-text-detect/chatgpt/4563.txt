Edit: I didn't expect this to get such a response. I've replied to some things and gave out some deltas, but as I won't have time to address everything, I'll close the thread now. Thanks for your input! This is my first post here at Codegolf so be gentle; Here's what I came up with in 15 minutes today (in Python): print sum (xy).find . 0 or 2int (regexp.compile d{2}," re.IGNORECASE x - y That question was from an online course on data science, so it has been edited several times since then:-D But anyway, that's not really important. The point of posting the code above is simply to illustrate how to solve that problem without using any external packageslibraries... Well, obviously there are solutions involving builtins like zip , len and splitting into lists etc., but they all take longer than writing a few lines of code, don't you think?! And if you're familiar with list comprehensions in other programming languages, why would you use any loops? Especially when those problems can be solved quite nicely by iterating over every pair of characters 'az', which takes about two seconds even on my old netbook. No matter whether we consider dictionary lookups, find calls, regexps, regular expressions, integer strings, string parsing, indexing and slicing operations etc.. It still takes roughly half the amount of time compared to just using a loop and running through each element once. Basically. So yeah, I guess I'm trying to say that most methods proposed before aren't nearly as efficient as one could initially think, especially when we compare them against simple code written directly in the language. Ofcourse there will always be exceptions, depending on where exactly the bottleneck lies, but generally speaking anything that involves evaluating something more complex than 0 1 values should rather be replaced by simpler alternatives by default. If someone doesn't know another way to do it, he might give up after only 5 seconds, whereas otherwise he may spend hours solving a problem because he overengineered it instead of coming up with a fast solution right away. For example, in our current case we'd need to write either lambda functions or nested for loops to count consecutive pairs of digits. That makes life easy for us since python treats lambdas pretty well, but on the other hand it also means we end up doing much more work. In contrast, we just need to traverse a sequence (the alphabet), checking its length at certain positions. We don't care what kind of structure these sequences have, nor do we want to treat them differently based on their contents or position within a larger text. There seem to be very little good reasons for this complexity. Moreover, I remember having seen similar questions being asked many years ago, for instance "how long does it take to chop off the last character?" So what did I come up with? This program prints the answer in under a second for small inputs. The algorithm works based on the observation that all decompositions of nonempty words can be obtained via concatenation of smaller pieces of constant size, ultimately giving us the correct result for infinite alphabets. At least that's what I believe judging by experience. Also note that this method requires no assumptions regarding the word frequency distribution or ordering, i.e. one letter combinations are equally likely as others. As Markov processes usually rely heavily on words vs single letters, I guess they wouldn't fit very well into this approach. Nevertheless, it worked surprisingly well given its simplicity. Any comments welcome;) A: Python 3, 46 bytes lambda s: " .join[s[1:] (ilen (s) -i) [::5]for i in range (min len (c) n) 3) or n) ] Try it online!