I believe that, at the end of the day, America (and to a lesser extent, the UK and USSR) ultimately lost the second World War in every way that matters. I think for many Americans this is difficult to accept because it means looking back on our own history as not being all-good or something worth glorifying - which isn't exactly how we were raised here in my opinion. Well said