I believe that, at the end of the day, America (and to a lesser extent, the UK and USSR) ultimately lost the second World War in every way that matters. The German defeat was not just an isolated event or even merely the result of a single individual's actions - rather it represented a profound shift for Europe as a whole which has proven impossible to reverse since then. To be clear: I do NOT mean "America defeated Germany by itself; therefore we won WWII." What I am saying is that had there been no United States in the first place, Hitler would have fallen much earlier than he did, without being able to build up his military power again after WW1 and lay dormant until 1938 when tensions were already high between European powers. Without US entry into WWII such a conflict may very well have ended differently. This does not mean that the USA stood on its own during the war- they quite clearly needed support from other key nations with strategic assets like oil and industry. But if you take away those countries entirely the reality changes dramatically.