# Discerning Differences: Synthetic Text and Human Text
As generative artificial intelligence (GenAI), exemplified by models like ChatGPT, gains prominence, it concurrently raises concerns about its potential misuse. This study
examines the effectiveness of synthetic text detection methods by
replicating and expanding upon the methodology from Shijaku
and Canhasi’s “ChatGPT Generated Text Detection” (2023). Utilizing their original dataset alongside the newly introduced DeepfakeTextDetect dataset, we applied traditional machine learning
models, such as XGBoost and Logistic Regression, to compare
the efficacy of custom lexical features against standard TF-IDF
vectorization. Our findings question the superiority of lexical
feature sets over simpler statistical methods, as no significant performance enhancements were observed. Furthermore, this study
explores the generalizability of these methods across different text
datasets, revealing considerable variances in model performance
when applied to unseen data. This research underscores the
need for ongoing refinement of text detection methodologies to
keep pace with advancing GenAI capabilities, highlighting the
potential of simpler, more robust feature extraction techniques
in enhancing synthetic text detection.
Index Terms—synthetic text detection, generative artificial
intelligence, natural language generation, dataset generalization,
cross validation, princicpal component analysis, logistic regression, linear discriminant analysis
<br>
<br>
**Paper**: [Discerning Differences: Synthetic Text and Human Text](https://github.com/Brian-D-Dang/Discerning-Differences-Synthetic-Text-and-Human-Text/blob/main/Discerning_Differences_Synthetic_Text_and_Human_Text.pdf)
